---
title: "Large Scale Language Models"
date: 2022-05-06
---

In the news: 

> "In line with Meta AIâ€™s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on 
publicly available data sets, to allow for more community engagement in understanding this foundational new technology."

References:
- https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/
- https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/
- https://github.com/facebookresearch/metaseq

Learning about transformers:
- Stanford CS224N Natural Language Processing with Deep Learning: https://web.stanford.edu/class/cs224n/
- Stanford CS224N Winter 2021 Video Lectures:  https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ
